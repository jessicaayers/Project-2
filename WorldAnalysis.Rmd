---
title: "Online News Popularity Analysis"
author: "Rina Deka and Jessica Ayers"
date: "2023-06-29"
---

```{r packages, message = FALSE}
library(tidyverse)
library(rmarkdown)
library(caret)
library(corrplot)
library(gbm)
library(randomForest)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.path="./images/")
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

This is an exploration of online news popularity data. For our analyses, we look at variables in the  data set such as the number of shares for online news articles, and variables that could be correlated with the amount of times an article is shared. For example, we looked at the rate of positive words used in an article (and its complement, the rate of negative words used in an article), which measures how often positive (or negative) words are found in an article. We also looked at global sentiment polarity, which is a metric of how polarized sentimentally the given text is; as well as title subjectivity, which measures how subjective a title is (is the title more like a scientific journal, or an opinion piece?). Other variables explored include number of images, and number of videos included in an article.

The purpose of this analysis was to explore relationships in the data to see if there are any promising predictors (features) for predicting the amount of times an online news article is shared. This was implemented by using linear regression models, as well as random forest and boosted tree models (which is an ensemble method).

# Data

```{r data, message = FALSE,warning=FALSE}
#import data set
#my data set is stored in the folder above where I have the project 2 folder
onpdata <- read_csv("../OnlineNewsPopularity/OnlineNewsPopularity.csv")
#create new variable to represent type of channel to use as parameters
for(i in 1:nrow(onpdata)){
  if(onpdata$data_channel_is_lifestyle[i] == 1){
    onpdata$channel[i] <- "Lifestyle"
  }
  else if(onpdata$data_channel_is_entertainment[i] == 1){
    onpdata$channel[i] <- "Entertainment"
  }
  else if(onpdata$data_channel_is_bus[i] == 1){
    onpdata$channel[i] <- "Bus"
  }
  else if(onpdata$data_channel_is_socmed[i] == 1){
    onpdata$channel[i] <- "Socmed"
  }
  else if(onpdata$data_channel_is_tech[i] == 1){
    onpdata$channel[i] <- "Tech"
  }
  else if(onpdata$data_channel_is_world[i] == 1){
    onpdata$channel[i] <- "World"
  }
}
```

```{r automation}
channelIDs <- unique(onpdata$channel)
output_file <- paste0(channelIDs, "Analysis.md")
params <- lapply(channelIDs, FUN = function(x){list(channel=x)})
analysis <- tibble(output_file, params)
```

```{r render, eval = FALSE}
for(i in 1:6){
onpdata_subset <- subset(onpdata, channel == params[[i]])
render(input = "WorldAnalysis.Rmd", output_file = output_file[i], params = params[i])
}
```

```{r subset}
unique(onpdata_subset$channel)
```

# Summarizations

```{r trainandtest, message = FALSE}
#create training and test data set
library(caret)
set.seed(111)
trainIndex <- createDataPartition(onpdata_subset$shares, p = 0.7, list = FALSE) 
dataTrain <- onpdata_subset[trainIndex, ]
dataTest <- onpdata_subset[-trainIndex, ]
```

```{r shares}
summary(dataTrain$shares)
```

The above summary provides the minimum, maximum, median, and mean of the number of shares in the training data. The first and third quantile are also included.

Since our response variable is the number of shares, we can first look at when the articles were published and the frequency for each day.

```{r days}
m <- sum(dataTrain$weekday_is_monday)
tu <- sum(dataTrain$weekday_is_tuesday)
wed <- sum(dataTrain$weekday_is_wednesday)
th <- sum(dataTrain$weekday_is_thursday)
f <- sum(dataTrain$weekday_is_friday)
sat <- sum(dataTrain$weekday_is_saturday)
sun <-sum(dataTrain$weekday_is_sunday)
data.frame(monday = m, tuesday = tu, wednesday = wed, thursday = th, friday = f, saturday = sat, sunday = sun)
days <- c(m, tu, wed, th, f, sat, sun)
```

From the above sums we can identify which day has the largest amount of published articles. We can visualize this by creating a bar graph as seen below.

```{r barplot}
barplot(days, main = "Frequency of published articles on each day", ylab = "Count", xlab = "Day",names.arg = c("Mon", "Tues", "Wed", "Thurs", "Fri", "Sat", "Sun"), col = "blue")
```

We can also look at different attributes that the articles have such as number of images and number of number of videos. We can explore if there are more shares with more images or videos.

```{r sum}
dataTrain %>%
  group_by(num_imgs, num_videos) %>%
  summarise(mean = mean(shares), sd = sd(shares))
```

Let's visualize this. First for number of images:

```{r plot1}
ggplot(dataTrain, aes(x = num_imgs, y = shares)) +
  geom_point() +
    geom_smooth(method = "lm", col = "green") + 
  geom_smooth() + 
   labs(title = "Number of Shares vs Number of Images", x = "Number of Images", y = "Number of Shares")
```

From the above plot if the trend shows a positive linear line, as number of images included in the article increases as does the number of shares. If it shows a negative linear line, the number of shares decreases with the addition of images. If no trend is shown, the number of images included has no impact on overall shares. 

For number of videos:

```{r plot2}
ggplot(dataTrain, aes(x = num_videos, y = shares)) +
  geom_point() + 
  geom_smooth(method = "lm", col = "green") + 
  geom_smooth() + 
  labs(title = "Number of Shares vs Number of Videos", x = "Number of Videos", y = "Number of Shares")
```

Similar to above, from the above plot if the trend shows a positive linear line, as number of videos included in the article increases as does the number of shares. If it shows a negative linear line, the number of shares decreases with the addition of videos If no trend is shown, the number of videos included has no impact on overall shares. 

```{r corr}
correlation <- cor(select(dataTrain, 2:13), method = "spearman")
corrplot(correlation, type = "upper", tl.pos = "lt")
correlation2 <-cor(select(dataTrain, 20:40))
corrplot(correlation2, type = "upper", tl.pos = "lt")
correlation3 <-cor(select(dataTrain, 41:60))
corrplot(correlation3, type = "upper", tl.pos = "lt")
```

For plots with larger, blue dots the two variables have a correlation close to +1. For plots with larger, red dots the two variables have a correlation close to -1. Both of these scenarios show signs of multicollinearity.

Furthermore, we investigated polarization in the news, as well as whether or not negative or positive sentiment news is shared more. 

Does it appear that news with a higher rate of positive words, or a higher rate of negative words, is shared more? 
Let's first take a look at the rate of positive words and shares.
```{r plot3}
#rate_negative_words

ggplot(dataTrain, aes(x=rate_positive_words, y=shares)) + 
  geom_line(col="green") + 
  labs(title = "Number of Shares by Rate of Positive Words", x = "Rate of Positive Words", y = "Number of Shares")
```

```{r plot4}
ggplot(dataTrain, aes(x=rate_negative_words, y=shares)) + 
  geom_line(col="red") + 
  labs(title = "Number of Shares by Rate of Negative Words", x = "Rate of Negative Words", y = "Number of Shares")
```

It seems that there is a spike in shares for online news that has mostly (about 75%) positive words, with the rest of the words being deemed negative. It seems that news with a lot of negative words don't get shared much, but news with positive words do.

Let's investigate this further. (Note: rate_negative_words is just the complement of rate_positive_words, but I did it twice for the sake of demonstration).

```{r shares_by_positive_words}
(shares_from_positive_words <- dataTrain %>%
  group_by(rate_positive_words) %>%
  summarise(mean = mean(shares), sd = sd(shares)))
```
```{r shares_by_negative_words}
(shares_from_negative_words <- dataTrain %>%
  group_by(rate_negative_words) %>%
  summarise(mean = mean(shares), sd = sd(shares)))
```

```{r highest_right_share}
rate_associated_with_highest_shares <- shares_from_positive_words %>%
  arrange(desc(mean))

rate_associated_with_highest_shares[1,]
```
It seems that the rate of positive words associated with the most average shares is about 71% (so 29% negative).

Is there a relationship between global sentiment polarity and the rate of negative words? What about title subjectivity and global sentiment polarity? And title subjectivity with rate of positive words?


```{r polarity_negativity_plot} 
ggplot(dataTrain,aes(x=rate_negative_words,y=global_sentiment_polarity)) + 
  geom_point(col="red") + 
  geom_smooth(method="lm") +
  labs(title = "Global Sentiment Polarity by Rate of Negative Words", x = "Rate of Negative Words", y = "Global Sentiment Polarity")
```


```{r subjectivity_polarity_plot}
ggplot(dataTrain,aes(x=title_subjectivity,y=global_sentiment_polarity)) + 
  geom_point(col="purple") + 
  geom_smooth(method="lm") +
  labs(title = "Global Sentiment Polarity by Rate of Negative Words", x = "Rate of Negative Words", y = "Global Sentiment Polarity")
```

```{r subjectivity_positivity_plot}
ggplot(dataTrain,aes(x=title_subjectivity,y=rate_positive_words)) + 
  geom_point(col="green") + 
  geom_smooth(method="lm") +
  labs(title = "Rate of Positive Words by Title Subjectivity", x = "Title Subjectivity", y = "Rate of Positive Words")
```
Overall, it doesn't seem like there's much of a relationship with the rate of positive words and title subjectivity, or title subjectivity with global sentiment polarity. However, at first glance there does seem to be a slightly negative relationship between the rate of negative words and global sentiment polarity at first glance but when taking a closer look it doesn't appear to be the case since there seems to be a lot of variation (points don't seem to "hug" the line and deviate qutie a bit from the regression line).

Are subjective titles shared more? What about articles with higher global sentiment polarity? Let's take a look but adjust the scale of shares so that it's a bit easier to visualize.

```{r log_shares_by_subjectivity_plot}
ggplot(dataTrain, aes(x=title_subjectivity, y=log(shares))) + 
  geom_jitter(col="pink") + 
  geom_smooth(method="lm") +
  labs(title = "Number of Shares by Title Subjectivity", x = "Title Subjectivity", y = "Number of Shares (Log Scale)")
```

```{r log_shares_by_polarity_plot}
ggplot(dataTrain, aes(x=global_sentiment_polarity, y=log(shares))) + 
  geom_jitter(col="cyan") + 
  geom_smooth(method="lm") +
  labs(title = "Number of Shares by Global Sentiment Polarity", x = "Global Sentiment Polarityy", y = "Number of Shares (Log Scale)")
```
There does not seem to be a linear relationship of shares with either global sentiment polarity or title subjectivity! 


# Modeling

### Linear Regression

A linear regression is composed of a response variable and different predictors. In a simple linear regression model, a single predictor is chosen to model a potential relationship with the corresponding response variable. In a multiple linear regression model, multiple predictors, including higher order terms, can be included to describe the relationship with the response variable. This kind of model is considered a branch of supervised learning since we choose a response variable. Our goal is to predict values of the response using our defined model. In this scenario, our response variable is the number of shares. We want to predict the number of shares based on the defined linear models. The first linear model uses all numeric variables in the provided data set as predictors. The data is standardized to ensure analysis is done consistently regardless of units. The second linear model includes all numeric variables and their squared counterparts. 

```{r preprocess}
dataTrain <- dataTrain %>% select(-starts_with("data_channel"),-url, -channel)
dataTest <- dataTest %>% select(-starts_with("data_channel"),-url, -channel)
preprocess <- preProcess(dataTrain, method = c("center", "scale"))
trainPreprocessed <- predict(preprocess, dataTrain)
testPreprocessed <- predict(preprocess, dataTest)
```

```{r linear1, warning = FALSE}
lmFit1<- train(shares ~ ., data = dataTrain, method = "lm",preProcess=c("center","scale"),
trControl = trainControl(method = "cv", number = 3))

summary(lmFit1)

lmFit1Pred <- predict(lmFit1, newdata = testPreprocessed)
lm1 <- postResample(lmFit1Pred, testPreprocessed$shares)
```

### Boosted Model

Boosted tree models are a type of tree-based model. We'll focus on specifically regression trees (as opposed to classification trees) here. For a regression tree, we predict a continuous response, we split up the predictor space into regions based on the mean of observations. These splits form the "branches" of the tree. The splits are chosen by fitting using recursive binary splitting, which is a "greedy" algorithm". For each value of a predictor, we find the SSE (residual sum of squares, or error sum of squares) and minimize it. This should allow us to obtain a split, and once that split is chosen, we do more iterations to create a second split, splits for that second split, and so on. This process grows a very large tree with many nodes, which we then "prune" using cost-complexity pruning. The pruning process prevents over-fitting of data. It does increase bias, but it decreases variance which should improve prediction.

Boosted trees are grown sequentially and reach subsequent tree is grown on a modified version of the original data. Sequential learning deals with errors created by previous trees; in boosting, new trees are formed by considering the errors of trees in previous rounds. Therefore, new trees are created one after another. Each tree is dependent on the previous tree. Predictions are updated as the trees are grown. 

How is this process implemented? Specifically, in the case of regression trees, the predictions are taken in initially as 0. Then, the residuals are found (observed/predicted difference). A tree is then fit with d splits, and d+1 terminal nodes, with the residuals treated as the response here. The predictions are then updated with a recursive definition: the updated prediction is equal to the old/initial prediction, plus the shrinkage parameter lambda (shrinkage parameter slows the fitting process down) multiplying the the predictions obtained from the fitting. We then continue to update residuals for new predictions for B times. Below, we used cross-validation to choose the parameters n, B lambda, as well as N (the number of trees).

```{r boosted1 ,warning=FALSE,echo=FALSE,cache=FALSE}
control <- trainControl(method = "cv",number = 3)
tuningGrid <- expand.grid(n.trees = c(25, 50, 100, 150, 200),
                          interaction.depth = 1:4,
                          shrinkage = 0.1, 
                          n.minobsinnode = 10)


boosted_tree_model <- train(shares ~ ., data = trainPreprocessed, method = "gbm", trControl = control, tuneGrid = tuningGrid)

```
```{r boosted2}
boosted_tree_model$bestTune
```

```{r boosted3}
boosted_tree_model$results
```

```{r boosted4}
boosted_tree_predictions <- predict(boosted_tree_model, newdata = testPreprocessed)

boosted_tree <- postResample(boosted_tree_predictions, testPreprocessed$shares)
boosted_tree
```

### Linear Regression 2

```{r linear2, cache = TRUE,warning=FALSE,message=FALSE}
lmFit2 <- train(shares ~ .^2, data = dataTrain, method = "lm",preProcess=c("center","scale"),
trControl = trainControl(method = "cv", number = 3))

summary(lmFit2)

lmFit2Pred <- predict(lmFit2, newdata = dataTest)
lm2 <- postResample(lmFit2Pred, dataTest$shares)
```

### Random Forest

Random forest models are an improvement and extension of bagging tree models. Instead of creating trees based off each variable chosen as a predictor, a range of values are fitted to find the optimal amount of trees for a random forest model. Similar to linear regression models, random forest models do very well with prediction, but tree based methods do not have the same level of interpretability. The random forest model below takes bootstrapped samples, fits subsets of 1 to 10 different predictors and the best fit number of trees will be returned. Ideally, a larger range for `mtry` would be used to run this model using cross validation, however the run time was extremely long and inefficient. A smaller range of 1 to 10 was chosen. The centered and scaled values of the data were also used in this model. 

```{r randomforest, cache=TRUE}
library(randomForest)
#using cross validation
#only used 1:10 to keep run time low
rfFit <- train(shares ~ ., data = trainPreprocessed, method = "rf", 
               trControl = trainControl(method = "cv", number = 3), 
               tuneGrid = data.frame(mtry = 1:10))
rfFit$results
rfFit$bestTune
rfPred <- predict(rfFit, newdata = testPreprocessed)
rf <- postResample(rfPred, testPreprocessed$shares)
```

## Comparison

```{r comparison}
df <- data.frame(linear1 = lm1, linear2 = lm2, boostedTree = boosted_tree, randomForest = rf)
dftransp <- as.data.frame(t(df))
dftransp <- dftransp %>%
  arrange(RMSE)
```

The winning model based on RMSE is:
```{r winner}
dftransp[1, ]
```


